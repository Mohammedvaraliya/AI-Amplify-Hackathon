{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\varal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the libraries for processing data and other utilities\n",
    "\n",
    "import os, sys, gc, warnings\n",
    "import logging, math, re, heapq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter    \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import words\n",
    "\n",
    "\n",
    "# This is to download stop words for cleaning the tweets\n",
    "import nltk\n",
    "nltk.download('words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = words.words()\n",
    "vocab = set(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 235892 key values pairs\n",
      "The count for the word 'thee' is 1\n"
     ]
    }
   ],
   "source": [
    "# Initiating the word_count dictionary and populating it\n",
    "word_count_dict = {}\n",
    "word_count_dict = Counter(word_list)\n",
    "print(f\"There are {len(word_count_dict)} key values pairs\")\n",
    "print(f\"The count for the word 'thee' is {word_count_dict.get('thee',0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of probs is 235892\n",
      "P('thee') is 0.0000\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Initalize the probability dictionary\n",
    "probs = {} \n",
    "total_words = sum(word_count_dict.values())\n",
    "\n",
    "for word, word_count in word_count_dict.items():\n",
    "    word_prob = word_count/total_words\n",
    "    probs[word] = word_prob\n",
    "print(f\"Length of probs is {len(probs)}\")\n",
    "\n",
    "# Let us use both the dictionaries for both word counts and probabilities and display an example word.\n",
    "print(f\"P('thee') is {probs['thee']:.4f}\")\n",
    "print(word_count_dict['thee'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"I live in New York and work as a data scientist.\")\n",
    "for word in doc.ents:\n",
    "    print(word.text, word.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_letter(word):\n",
    "    delete_list = []\n",
    "    split_list = []\n",
    "    split_list = [(word[:i], word[i:]) for i in range(len(word))]\n",
    "    delete_list = [L+R[1:] for L, R in split_list]\n",
    "    return delete_list\n",
    "\n",
    "def switch_letter(word):\n",
    "    switch_list = []\n",
    "    split_list = []\n",
    "    split_list = [(word[:i], word[i:]) for i in range(len(word))]\n",
    "    switch_list = [L + R[1] + R[0] + R[2:] for L, R in split_list if len(R)>=2]\n",
    "    return switch_list\n",
    "\n",
    "def replace_letter(word):\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    replace_list = []\n",
    "    split_list = []\n",
    "    split_list = [(word[0:i], word[i:]) for i in range(len(word))]\n",
    "    replace_list = [L + letter + (R[1:] if len(R)>1 else '') for L, R in split_list if R for letter in letters]\n",
    "    replace_set = set(replace_list)\n",
    "    replace_list = sorted(list(replace_set))   \n",
    "    return replace_list\n",
    "\n",
    "def insert_letter(word):\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    insert_list = []\n",
    "    split_list = []\n",
    "    split_list = [(word[0:i], word[i:]) for i in range(len(word)+1)]\n",
    "    insert_list = [L + letter + R for L, R in split_list for letter in letters]\n",
    "    return insert_list\n",
    "\n",
    "def edit_one_letter(word, allow_switches = True):\n",
    "    edit_one_set = set()\n",
    "    edit_one_set.update(delete_letter(word))\n",
    "    if allow_switches: edit_one_set.update(switch_letter(word))\n",
    "    edit_one_set.update(replace_letter(word))\n",
    "    edit_one_set.update(insert_letter(word))\n",
    "    if word in edit_one_set: edit_one_set.remove(word)\n",
    "    return edit_one_set\n",
    "\n",
    "def edit_two_letter(word, allow_switches = True):\n",
    "    edit_two_set = set()\n",
    "    edit_one = edit_one_letter(word, allow_switches=allow_switches)\n",
    "    for word in edit_one:\n",
    "        if word:\n",
    "            edit_two = edit_one_letter(word, all,  ow_switches=allow_switches)\n",
    "            edit_two_set.update(edit_two)\n",
    "    \n",
    "    return edit_two_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spelling_suggestions(word, probs, vocab, n=2,):    \n",
    "    suggestions = []\n",
    "    top_n_suggestions = []\n",
    "    suggestions = list((word in vocab and word) or \n",
    "                       edit_one_letter(word).intersection(vocab) or\n",
    "                       edit_two_letter(word).intersection(vocab))\n",
    "    top_n_suggestions = [[s, probs[s]] for s in list(suggestions)]\n",
    "    return top_n_suggestions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Word - sectin\n",
      "word - 0: pectin, probability 0.000004\n",
      "word - 1: section, probability 0.000004\n",
      " \n",
      "Word - condtion\n",
      "word - 0: conation, probability 0.000004\n",
      "word - 1: condition, probability 0.000008\n",
      " \n",
      "Word - condotin\n",
      "word - 0: conjoin, probability 0.000004\n",
      "word - 1: condoling, probability 0.000004\n",
      "word - 2: condition, probability 0.000008\n",
      " \n",
      "Word - disdaain\n",
      "word - 0: disdain, probability 0.000004\n",
      " \n",
      "Word - tumtultous\n",
      "word - 0: tumulous, probability 0.000004\n",
      "word - 1: tumultuous, probability 0.000004\n"
     ]
    }
   ],
   "source": [
    "my_words = ['sectin','condtion','condotin','disdaain','tumtultous']\n",
    "tmp_corrections = []\n",
    "for word_c in my_words: \n",
    "    tmp_corrections.append(get_spelling_suggestions(word_c, probs, vocab, 3))\n",
    "for i, word in enumerate(my_words):\n",
    "    print(' ')\n",
    "    print(f'Word - {my_words[i]}')\n",
    "    for j, word_prob in enumerate(tmp_corrections[i]):\n",
    "        print(f\"word - {j}: {word_prob[0]}, probability {word_prob[1]:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73d9578fe47b5ba5fd84260a9ac8b0ee16026ba66ba23d711b61c8b48635e5ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
